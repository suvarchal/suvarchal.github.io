import{_ as r}from"./slidev/VClick-q7zWezGA.js";import{b as a,o as u,w as l,g as e,e as p,ad as d,v as m,x as f,T as o}from"./modules/vue-ICVoADI0.js";import{I as c}from"./slidev/default-CL1Cu9C5.js";import{u as x,f as v}from"./slidev/context-DoYRliGO.js";import"./index-B2AVM5XZ.js";import"./modules/shiki-Cr0GD95B.js";import"./slidev/VClicks-BkzfaFdQ.js";const h={__name:"04-slide.md__slidev_8",setup(y){const{$clicksContext:n,$frontmatter:i}=x();return n.setup(),(g,t)=>{const s=r;return u(),a(c,m(f(o(v)(o(i),7))),{default:l(()=>[t[1]||(t[1]=e("h1",null,"Archetype Dwarf: Results",-1)),t[2]||(t[2]=e("ul",null,[e("li",null,[e("p",null,"âœ… Can produce bit identical results as FESOM2 dwarf.")]),e("li",null,[e("p",null,"Initially implemented with pure JAX and mpi4py for exchanges on multi-host: ~100x slower on CPUs; in and out of jit compilation for MPI exchanges.")]),e("li",null,[e("p",null,"mpi4jax improves performance and also works on multi-node CPUs ~10x slower but GPU induces complex installation toolchain.")]),e("li",null,[e("p",null,[e("strong",null,"Illustrates further viability of using JAX for FESOMx-like operations")])])],-1)),t[3]||(t[3]=e("h2",null,"Issues:",-1)),t[4]||(t[4]=e("ul",null,[e("li",null,"mpi4py, mpi4jax -> jax(version) && mpi4py (with cuda compatible MPI). mpi4jaxn is in-frequently updated and is tied to JAX version, Complex HPC and local development setup."),e("li",null,"JAX-XLA uses all cores by default, complex interplay with external MPI."),e("li",null,"Grids still inflexible, compute kernel infrastructure is too tied to current FESOM2: partitions, loops over edges and faces, any vectorization issues of FESOM2 are carried over, not seamless integration for hybrid models.")],-1)),p(s,null,{default:l(()=>[...t[0]||(t[0]=[d(" **Is evolving Jax-native distributed and sharding, integrated with SLURM scheduler any better?** ",-1)])]),_:1})]),_:1},16)}}};export{h as default};
